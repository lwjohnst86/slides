{
  "hash": "ae040d8d5f6833396a516145d8a70b12",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Denmark Statistics and large health data: We don't know what we're doing\"\ndate: \"2025-09-25\"\nlocation: \"Department of Urology, Aarhus University Hospital, Denmark\"\n---\n\n::: content-hidden\nWe think it could be interesting to hear about you perspectives on\nworking with Statistics Denmark and your views on working with health\ndata on a large scale. Does this make sense to you?\n\nMy experience with working on DST has largely been much more on the\ntechnical/programming side (how to do analyses much faster, etc), on the\nreproducibility/validity of scientific research (in general but also\nspecific to working with larger data), and a lot of ranting about how\nDST decided to organise their data (it is very out-dated),\n\n45 minutes \\~ 40 slides?\n:::\n\n# Who am I? :wave: :wave:\n\n## History {.center}\n\n-   MSc and PhD in Nutritional Science in Toronto, Canada\n\n-   (Previous) Research in diabetes epidemiology\n\n-   Team leader at SDCA for the [Seedcase\n    Project](https://seedcase-project.org), an NNF funded software\n    project to simplify building FAIR data\n\n::: notes\nI started out doing more \"traditional\" research but have gradually been\nmoving towards more computationally technical work, like software\ndevelopment and data engineering.\n\nAs I continued my research career, I got more and more into building\nsoftware and creating courses to teach researchers about better\nprogramming practices. This also meant that I had less and less time to\npublish papers, because, there's only so much time in a day.\n\nAs I go along, keep this in mind, because it highlights a big issue we\nhave in research.\n:::\n\n## My work on large data {.center}\n\n-   [ukbAid](https://steno-aarhus.github.io/ukbAid/): R package and\n    website\n\n-   [DARTER Project](https://steno-aarhus.github.io/darter-project/):\n    Website of application to and documentation on a DST project\n\n# Rationale for this talk {.center}\n\n::: aside\nAside from being asked to present on it :zany_face:\n:::\n\n## We are woefully behind on data engineering and programming practices {.center}\n\n::: aside\nEspecially when working with massive data\n:::\n\n## This impacts how we do effective and accurate research {.center}\n\n## This is important because: {.center}\n\n-   Validity of results\n\n-   Speed impacts time to results\n\n-   Ability to do more complex analysis with more data\n\n-   Resources (which cost money)\n\n# Aim of this talk {.center}\n\n1.  Highlight issues with Denmark Statistics and the research using the\n    registers (and any other large data)\n\n2.  Spread awareness of how critical programming skills are, especially\n    for large data\n\n3.  Showcase a few tools for doing research faster, so you can focus on\n    doing science\n\n::: notes\nAlso keep this very informal, ask questions at any point.\n\nThere's a lot of issues with how DST is organised and structured. I\nwon't go into all of them, but instead will focus on some of the more\nbasic, fundamental ones.\n:::\n\n## General \"roadmap\" :world_map: {.center}\n\n-   Issues with DST\n\n-   Need for dedicated programming expertise\n\n-   Questionable reproducibility and validity\n\n# Denmark Statistics, the good *and* the bad\n\n::: aside\nAnyone do research with DST?\n:::\n\n## The good: Amazing resource, gold mine of data {.center}\n\n::: aside\n*Nearly* unique in the world!\n:::\n\n## And the bad... {.center}\n\n::: aside\nFor this talk, only basic things\n:::\n\n::: notes\nApplication alone is a massive barrier to working with the data, but I\nwon't talk about that in this talk, just to highlight that it is a big\nissue.\n\nAll of these things are important to remember later on in the\npresentation.\n:::\n\n## Everyone works on same server {.center}\n\n-   No queue system for analyses\n\n-   Can crash your analysis if others use more memory\n\n::: aside\n*Waste of resources, slows research*\n:::\n\n::: notes\nSo that basically means, it's best to work on the weekends or evenings\nfor simple stuff. And hope that for longer analyses, no one else hoards\nall the memory and crashing your analysis.\n\nRemember how I mentioned about why this is important? This is a massive\nwaste of resources and time and effort, if analyses just crash because\nsomeone else is using too much memory.\n\nIt also slows down research because of something so stupid as this.\n:::\n\n## Everyone works in same folder in a project {.center}\n\n::::: columns\n::: column\n``` text\nproject/\n├── luke/\n│   └── analysis/\n└── omar/\n    └── paper/\n```\n:::\n\n::: {.column .fragment}\nCollaboration is difficult:\n\n-   *Anyone* can edit anything\n-   Can't know who changed what and when (no \"version control\")\n-   Not easy to review and improve other's code\n:::\n:::::\n\n::: aside\nSlows research down\n:::\n\n::: notes\nNo version control or separation of sub-projects.\n\nI'll come back to this reviewing thing later.\n:::\n\n## Data is stored in a proprietary SAS format {.center}\n\nFor example, BEF register:\n\n``` text\nbef2018.sas7bdat\nbef2019.sas7bdat\nbef2020.sas7bdat\nbef2021.sas7bdat\nbef2022.sas7bdat\n```\n\n. . .\n\nTakes many minutes to load one year of data (in R)\n\n::: aside\n*Waste of time and resources*\n:::\n\n::: notes\nThis means, if you use R, or Python, or Stata, you have to load these,\nwhich can take many minutes per file, just to load it.\n\nThese are important to know for later.\n:::\n\n## Data updates make more work for us {.center}\n\n``` text\nbef2021.sas7bdat\nbef2022.sas7bdat\nDecember_2023/bef2022.sas7bdat\nDecember_2023/bef2023.sas7bdat\n```\n\n**Can you see the issue?**\n\n. . .\n\n::: aside\n*Again, waste of time*\n:::\n\n::: notes\nOne problem, sometimes there's a new version of a year you already had.\nBut you don't know what's been changed. You have to spend time checking\nwhat changed and if it messes things up for you. The second problem is,\nthe updates are in a new folder. So trying to build an automated\npipeline to load the data in is a bit of a pain because the structure\nchanges for each update.\n\nThese seem like small things, but these are the things that mess up how\ncode works and runs. These things are what make it hard to build\neffective analyses and effective software to ideally simplify your life\nwhile working with the registers.\n:::\n\n## Metadata is confusing and poorly documented {.center}\n\n-   Variables are not consistent across years\n\n-   Finding the metadata is difficult\n\n::: aside\n*Slows research down, wastes time*\n:::\n\n::: notes\nMetadata is a big problem. Documentation is relatively poor for most of\nthe variables, it's in another location that requires you to dig into\nit. Values in some variables that are numbers but actually are\ncategories... but the documentation for what those numbers mean isn't in\nthe same place. So requires searching.\n:::\n\n## DST is either unaware of or indifferent to improving things {.center}\n\n::: notes\nThere's so much potential there, but DST doesn't seem to value research\nand improvements. Or at least, from their actions and infrastructure so\nfar, it doesn't seem like it.\n:::\n\n## Highlights lack of engineering and design expertise at DST {.center}\n\n-   Puts tech burden onto researchers\n\n# Need programming expertise, especially for large data\n\n::: aside\nAn organization should be responsible for having a well-designed setup,\notherwise, its on the researcher. But that's not their expertise.\n:::\n\n## Two tools as examples: Parquet and DuckDB {.center}\n\n::: aside\n<https://parquet.apache.org/>\n\n<https://duckdb.org/>\n:::\n\n## Parquet should be used to store large data {.center}\n\nMost data formats are row-based, like CSV or SAS. Newer formats tend to\nbe column-based like Parquet.\n\n. . .\n\n::::: columns\n::: column\n### Row-based\n\n``` text\nname,sex,age\nTim,M,30\nJenny,F,25\n```\n:::\n\n::: {.column .fragment}\n### Column-based\n\n``` text\nname,Tim,Jenny\nsex,M,F\nage,30,25\n```\n:::\n:::::\n\n## Column-based storage has many advantages\n\n::::: columns\n::: column\n### Compression\n\n``` text\nname,Tim,Sam,Jenny\nsex,M,F,F\nage,30,30,25\n```\n\n...becomes...\n\n``` text\nname,Tim,Jenny,Sam\nsex,M,F{2}\nage,30{2},25\n```\n:::\n\n::: {.column .fragment}\n### Loading\n\n-   Computers read by lines\n-   Per line = same data type\n-   Only read needed columns\n\n``` text\nsex,M,F\nage,30,25\n```\n:::\n:::::\n\n## Parquet is 50-75% smaller than other formats {.center}\n\n| File type            | Size (MB)    |\n|----------------------|--------------|\n| SAS (`.sas7bdat`)    | 1.45 Gb      |\n| CSV (`.csv`)         | \\~90% of SAS |\n| Stata (`.dta`)       | 745 Mb       |\n| Parquet (`.parquet`) | 398 Mb       |\n\n: File size between CSV, Parquet, Stata, and SAS for `bef` register for\n2017.\n\n::: notes\nWhy doesn't DST use Parquet? It's a clearly better format. Unless they\nkeep SAS because it uses so much storage, which they charge for.\n\ne.g. when someone uses Stata, they need to copy the data to the folder\nand convert to Stata format... which basically doubles the amount of\nstorage used... thus increasing costs for the researcher.\n\nMy own experience: 500 GB of SAS data converted to Parquet is only 80\nGB.\n:::\n\n## Can partition data by a value (e.g. year) and load all at once {.center}\n\n``` text\nbef/year=2018/part-0.parquet\nbef/year=2019/part-0.parquet\nbef/year=2020/part-0.parquet\nbef/year=2021/part-0.parquet\nbef/year=2022/part-0.parquet\n```\n\n. . .\n\nLoad in R with `arrow` package:\n\n``` r\nbef <- arrow::open_dataset(\"bef\")\n```\n\n> Load all years in \\< 1 sec, compared to \\~5 min for one year from SAS\n> format\n\n## DuckDB is a recent SQL engine designed for analytical queries {.center}\n\n::: aside\n<https://duckdb.org/>\n:::\n\n## SQL, or Structured Query Language, is a language for managing and querying databases {.center}\n\n## DuckDB is impressively fast {.center}\n\n-   Faster than almost all other tools\n\n    -   Relatively complex queries (joins, group by, aggregates) on 55\n        Gb takes \\< 7.5 min [^1]\n\n    -   Generally, simpler queries take \\< 10 seconds for massive\n        datasets\n\n-   Easily connects with Parquet datasets\n\n[^1]: From benchmark [page](https://duckdblabs.github.io/db-benchmark/).\n\n## Example in DST {.center}\n\n1.  Load all 45 years of BEF\n2.  Drop all missing PNR\n3.  Group by year\n4.  Count sex\n\n. . .\n\n**Takes \\< 6 seconds**\n\n## So... why is this important? Seems like easy stuff! {.center}\n\n-   Because it highlights a lack of expertise and understanding of some\n    basic things\n\n# Reproducibility and verification of research\n\nReproducibility: same data + same code = same results?\n\n::: notes\nBackground on reproducibility, explain what\n\nWhich ties into reproducibility, which is when the same data and the\nsame code are run independently of the original authors, to get the same\nresults.\n\nBut if the expertise for coding, for basic understandings of computeres\nand data, are lacking, it's difficult to actually ensure\nreproducibility.\n:::\n\n## Non-reproducibility is a *big*, though mostly unknown, issue {.center}\n\n::: {layout-ncol=\"2\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n:::\n\n[DOI:\n10.1093/gigascience/giad113](https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad113/7516267?login=false)\n\n## Analyzing, and reproducing, large data requires programming skills {.center}\n\n-   With massive data, you really need to know how to code and\n    programming\n\n-   But, researchers are not trained for this kind of skill\n\n::: notes\nHoping your skills and knowledge are enough to do the analysis, without\nactually knowing if they are good enough.\n:::\n\n## No one reviews code, so, no one knows if it's correct {.center}\n\n-   This problem is bigger than people realize.\n\n::: aside\nOr rather, *very* few formally review code.\n:::\n\n::: notes\nWho has reviewed code here, like gone through all of it, not just looked\nat it?\n\nI've done a few formal code reviews and ... it's not good. I had to stop\nreviewing things because it was just too much work and too stressful.\nUntil a cultural change happens, I'm very pessimistic about things.\nGiven code that I've seen so far, I think this is a bigger problem to\nthe accuracy of research from registers than a lot of people realise.\n\nI get it, researchers are not taught these things. Nor do they have a\nlot of time, they have so many things to do. In my team, where we build\nsoftware, we review every single change or addition to the code. It's\nhard work. It takes a lot of time to learn to make some changes that are\neasy to review. And it takes a lot of time to understand what the person\nis trying to do with the code.\n\nAnd this is with three of us, including me, who have a lot of\nprogramming and software development experience.\n:::\n\n## \"Science as amateur software development\" {.center}\n\n> \"But the code runs!\"\n\n::: aside\nFrom this academic\n[presentation](https://www.youtube.com/watch?v=8qzVV7eEiaI).\n:::\n\n::: notes\nThe state of programming and software development in research is abysmal\nto say the least.\n\nThe problem that researchers aren't even really aware of this problem No\nfunding goes to it, there are few courses on it, it isn't required, we\ncannot hire skilled personnel for this either because it isn't funded\nnor valued. What ends up happening? You get numbers from running the\nanalysis... But now do you even know it's right? Programming is *hard*,\nway harder than doing research.\n:::\n\n## Working in DST makes reproducibility harder {.center}\n\n::: {.columns}\n::: {.column}\n- Hard to review code\n\n-   Hard to collaborate\n\n-   No version control\n:::\n::: {.column}\n\n-   Use proprietary data formats\n\n-   No queue system\n\n-   No training materials or courses\n:::\n:::\n\n::: notes\nSo makes it harder to have several people working on a project, for\nexample, if one person has more expertise in programming/coding.\n:::\n\n## Science is not about trust, it's about verification {.center}\n\n::: aside\nSome trust is needed, but shouldn't be dependent on it.\n:::\n\n::: notes\nThe whole point of science is verification. Otherwise we are no\ndifferent from pseudoscience. If we can't verify that the results are\ncorrect, by looking at the code and reviewing it and running it, how can\nwe know and trust it?\n\nMention tho that I believe knowledge generated from science is still\nbetter than things made up out of thin air.\n:::\n\n## It's difficult to trust what researchers do in DST {.center}\n\n# What to do?\n\n## If reviewing papers, request or demand code is accessible {.center}\n\n::: aside\nYou don't have to review it, just ask for it.\n:::\n\n## Recognize, value, and reward those with programming expertise {.center}\n\n## Pressure DST to improve things {.center}\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}